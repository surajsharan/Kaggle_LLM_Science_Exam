{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Platypus2-70B + Wikipedia RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Installing offline dependencies\n",
    "# !pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "# !pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import ctypes\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For RAG\n",
    "import faiss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "NUM_TITLES = 5\n",
    "MAX_SEQ_LEN = 512\n",
    "MODEL_PATH = \"wikipedia/bge-small-faiss/\"\n",
    "\n",
    "# For LLM\n",
    "from transformers import AutoConfig, AutoModelForCausalLM,AutoModelForMultipleChoice, AutoTokenizer, AutoModel\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils.modeling import set_module_tensor_to_device\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "N_BATCHES = 5 \n",
    "MAX_CONTEXT = 1956\n",
    "MAX_LENGTH = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean RAM & vRAM\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load data\n",
    "# df = pd.read_csv(\"input_data/train_with_context2.csv\",index_col='id')\n",
    "\n",
    "df = pd.read_csv(\"input_data/train_with_context2.csv\")\n",
    "df['id'] = range(len(df))\n",
    "\n",
    "# Variable used to avoid running the notebook for 3 hours when submitting. Credit : CPMP\n",
    "IS_TEST_SET = True\n",
    "\n",
    "# Uncomment this to see results on the train set\n",
    "# df = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/train.csv\", index_col=\"id\")\n",
    "# IS_TEST_SET = True\n",
    "# N_BATCHES = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"6,9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.set_index('id',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wikipedia Retrieval Augmented Generation (RAG)\n",
    "\n",
    "The following code is adapted from [the notebook of @MGöksu](https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model) and [the notebook of @MB](https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles/notebook). We use the [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) to embed the Wikipedia dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New SentenceTransformer class similar to the one used in @Mgöksu notebook but relying on the transformers library only\n",
    "\n",
    "class SentenceTransformer:\n",
    "    def __init__(self, checkpoint, device=\"cuda:0\"):\n",
    "        self.device = device\n",
    "        self.checkpoint = checkpoint\n",
    "        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    def transform(self, batch):\n",
    "        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n",
    "        return tokens.to(self.device)  \n",
    "\n",
    "    def get_dataloader(self, sentences, batch_size=32):\n",
    "        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n",
    "        dataset = Dataset.from_dict({\"text\": sentences})\n",
    "        dataset.set_transform(self.transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        return dataloader\n",
    "\n",
    "    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n",
    "        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n",
    "        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n",
    "\n",
    "        embeddings = []\n",
    "        for batch in pbar:\n",
    "            with torch.no_grad():\n",
    "                e = self.model(**batch).pooler_output\n",
    "                e = F.normalize(e, p=2, dim=1)\n",
    "                embeddings.append(e.detach().cpu().numpy())\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prompt embedding, t=0.0s\n",
      "Loading faiss index, t=5.2s\n",
      "Starting text search, t=7.9s\n",
      "Starting context extraction, t=33.3s\n",
      "Context added, t=34.2s\n"
     ]
    }
   ],
   "source": [
    "if IS_TEST_SET:\n",
    "    # Load embedding model\n",
    "    start = time()\n",
    "    print(f\"Starting prompt embedding, t={time() - start :.1f}s\")\n",
    "    model = SentenceTransformer(MODEL_PATH, device=\"cuda:0\")\n",
    "\n",
    "    # Get embeddings of prompts\n",
    "    f = lambda row : \" \".join([row[\"prompt\"], row[\"A\"], row[\"B\"], row[\"C\"], row[\"D\"], row[\"E\"]])\n",
    "    inputs = df.apply(f, axis=1).values # better results than prompt only\n",
    "    prompt_embeddings = model.encode(inputs, show_progress_bar=False)\n",
    "\n",
    "    # Search closest sentences in the wikipedia index \n",
    "    print(f\"Loading faiss index, t={time() - start :.1f}s\")\n",
    "    faiss_index = faiss.read_index(MODEL_PATH + '/faiss.index')\n",
    "    # faiss_index = faiss.index_cpu_to_all_gpus(faiss_index) # causes OOM, and not that long on CPU\n",
    "\n",
    "    print(f\"Starting text search, t={time() - start :.1f}s\")\n",
    "    search_index = faiss_index.search(np.float32(prompt_embeddings), NUM_TITLES)[1]\n",
    "\n",
    "    print(f\"Starting context extraction, t={time() - start :.1f}s\")\n",
    "    dataset = load_from_disk(\"wikipedia/all-paraphs-parsed-expanded\")\n",
    "    for i in range(len(df)):\n",
    "        df.loc[i, \"context_new\"] = \"-\" + \"\\n-\".join([dataset[int(j)][\"text\"] for j in search_index[i]])\n",
    "\n",
    "    # Free memory\n",
    "    faiss_index.reset()\n",
    "    del faiss_index, prompt_embeddings, model, dataset\n",
    "    clean_memory()\n",
    "    print(f\"Context added, t={time() - start :.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>answer</th>\n",
       "      <th>id</th>\n",
       "      <th>context_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>The presence of a clustered thick disk-like co...</td>\n",
       "      <td>MOND is a theory that reduces the observed mis...</td>\n",
       "      <td>MOND is a theory that increases the discrepanc...</td>\n",
       "      <td>MOND is a theory that explains the missing bar...</td>\n",
       "      <td>MOND is a theory that reduces the discrepancy ...</td>\n",
       "      <td>MOND is a theory that eliminates the observed ...</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>-MOND is an example of a class of theories kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which of the following is an accurate definiti...</td>\n",
       "      <td>Many of these systems evolve in a self-similar...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>Dynamic scaling refers to the non-evolution of...</td>\n",
       "      <td>Dynamic scaling refers to the evolution of sel...</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>-In such systems we can define a certain time-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>It is possible that this usage is related with...</td>\n",
       "      <td>The triskeles symbol was reconstructed as a fe...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>The triskeles symbol is a representation of a ...</td>\n",
       "      <td>The triskeles symbol represents three interloc...</td>\n",
       "      <td>The triskeles symbol is a representation of th...</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>-Classical Antiquity The triskeles proper, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the significance of regularization in ...</td>\n",
       "      <td>Renormalization is distinct from regularizatio...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>Regularizing the mass-energy of an electron wi...</td>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>-Regularization: Classical physics theory brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which of the following statements accurately d...</td>\n",
       "      <td>Several qualitative observations can be made o...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>The angular spacing of features in the diffrac...</td>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>-Several qualitative observations can be made ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>What is the relation between the three moment ...</td>\n",
       "      <td>The second equation is more general as it does...</td>\n",
       "      <td>The three moment theorem expresses the relatio...</td>\n",
       "      <td>The three moment theorem is used to calculate ...</td>\n",
       "      <td>The three moment theorem describes the relatio...</td>\n",
       "      <td>The three moment theorem is used to calculate ...</td>\n",
       "      <td>The three moment theorem is used to derive the...</td>\n",
       "      <td>C</td>\n",
       "      <td>195</td>\n",
       "      <td>-In civil engineering and structural analysis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>What is the throttling process, and why is it ...</td>\n",
       "      <td>A throttle is the mechanism by which fluid flo...</td>\n",
       "      <td>The throttling process is a steady flow of a f...</td>\n",
       "      <td>The throttling process is a steady adiabatic f...</td>\n",
       "      <td>The throttling process is a steady adiabatic f...</td>\n",
       "      <td>The throttling process is a steady flow of a f...</td>\n",
       "      <td>The throttling process is a steady adiabatic f...</td>\n",
       "      <td>B</td>\n",
       "      <td>196</td>\n",
       "      <td>-Throttling One of the simple applications of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>What happens to excess base metal as a solutio...</td>\n",
       "      <td>Furthermore, this melting may begin at a tempe...</td>\n",
       "      <td>The excess base metal will often solidify, bec...</td>\n",
       "      <td>The excess base metal will often crystallize-o...</td>\n",
       "      <td>The excess base metal will often dissolve, bec...</td>\n",
       "      <td>The excess base metal will often liquefy, beco...</td>\n",
       "      <td>The excess base metal will often evaporate, be...</td>\n",
       "      <td>B</td>\n",
       "      <td>197</td>\n",
       "      <td>-Similarly, a hypoeutectoid alloy has two crit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>What is the relationship between mass, force, ...</td>\n",
       "      <td>Newton first set out the definition of mass Th...</td>\n",
       "      <td>Mass is a property that determines the weight ...</td>\n",
       "      <td>Mass is an inertial property that determines a...</td>\n",
       "      <td>Mass is an inertial property that determines a...</td>\n",
       "      <td>Mass is an inertial property that determines a...</td>\n",
       "      <td>Mass is a property that determines the size of...</td>\n",
       "      <td>D</td>\n",
       "      <td>198</td>\n",
       "      <td>-Mass is (among other properties) an inertial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>What did Arthur Eddington discover about two o...</td>\n",
       "      <td>Eddington's criticism seems to have been based...</td>\n",
       "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
       "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
       "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
       "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
       "      <td>Arthur Eddington showed that two of Einstein's...</td>\n",
       "      <td>C</td>\n",
       "      <td>199</td>\n",
       "      <td>-The possibility of gravitational waves was di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0    Which of the following statements accurately d...   \n",
       "1    Which of the following is an accurate definiti...   \n",
       "2    Which of the following statements accurately d...   \n",
       "3    What is the significance of regularization in ...   \n",
       "4    Which of the following statements accurately d...   \n",
       "..                                                 ...   \n",
       "195  What is the relation between the three moment ...   \n",
       "196  What is the throttling process, and why is it ...   \n",
       "197  What happens to excess base metal as a solutio...   \n",
       "198  What is the relationship between mass, force, ...   \n",
       "199  What did Arthur Eddington discover about two o...   \n",
       "\n",
       "                                               context  \\\n",
       "0    The presence of a clustered thick disk-like co...   \n",
       "1    Many of these systems evolve in a self-similar...   \n",
       "2    It is possible that this usage is related with...   \n",
       "3    Renormalization is distinct from regularizatio...   \n",
       "4    Several qualitative observations can be made o...   \n",
       "..                                                 ...   \n",
       "195  The second equation is more general as it does...   \n",
       "196  A throttle is the mechanism by which fluid flo...   \n",
       "197  Furthermore, this melting may begin at a tempe...   \n",
       "198  Newton first set out the definition of mass Th...   \n",
       "199  Eddington's criticism seems to have been based...   \n",
       "\n",
       "                                                     A  \\\n",
       "0    MOND is a theory that reduces the observed mis...   \n",
       "1    Dynamic scaling refers to the evolution of sel...   \n",
       "2    The triskeles symbol was reconstructed as a fe...   \n",
       "3    Regularizing the mass-energy of an electron wi...   \n",
       "4    The angular spacing of features in the diffrac...   \n",
       "..                                                 ...   \n",
       "195  The three moment theorem expresses the relatio...   \n",
       "196  The throttling process is a steady flow of a f...   \n",
       "197  The excess base metal will often solidify, bec...   \n",
       "198  Mass is a property that determines the weight ...   \n",
       "199  Arthur Eddington showed that two of Einstein's...   \n",
       "\n",
       "                                                     B  \\\n",
       "0    MOND is a theory that increases the discrepanc...   \n",
       "1    Dynamic scaling refers to the non-evolution of...   \n",
       "2    The triskeles symbol is a representation of th...   \n",
       "3    Regularizing the mass-energy of an electron wi...   \n",
       "4    The angular spacing of features in the diffrac...   \n",
       "..                                                 ...   \n",
       "195  The three moment theorem is used to calculate ...   \n",
       "196  The throttling process is a steady adiabatic f...   \n",
       "197  The excess base metal will often crystallize-o...   \n",
       "198  Mass is an inertial property that determines a...   \n",
       "199  Arthur Eddington showed that two of Einstein's...   \n",
       "\n",
       "                                                     C  \\\n",
       "0    MOND is a theory that explains the missing bar...   \n",
       "1    Dynamic scaling refers to the evolution of sel...   \n",
       "2    The triskeles symbol is a representation of a ...   \n",
       "3    Regularizing the mass-energy of an electron wi...   \n",
       "4    The angular spacing of features in the diffrac...   \n",
       "..                                                 ...   \n",
       "195  The three moment theorem describes the relatio...   \n",
       "196  The throttling process is a steady adiabatic f...   \n",
       "197  The excess base metal will often dissolve, bec...   \n",
       "198  Mass is an inertial property that determines a...   \n",
       "199  Arthur Eddington showed that two of Einstein's...   \n",
       "\n",
       "                                                     D  \\\n",
       "0    MOND is a theory that reduces the discrepancy ...   \n",
       "1    Dynamic scaling refers to the non-evolution of...   \n",
       "2    The triskeles symbol represents three interloc...   \n",
       "3    Regularizing the mass-energy of an electron wi...   \n",
       "4    The angular spacing of features in the diffrac...   \n",
       "..                                                 ...   \n",
       "195  The three moment theorem is used to calculate ...   \n",
       "196  The throttling process is a steady flow of a f...   \n",
       "197  The excess base metal will often liquefy, beco...   \n",
       "198  Mass is an inertial property that determines a...   \n",
       "199  Arthur Eddington showed that two of Einstein's...   \n",
       "\n",
       "                                                     E answer   id  \\\n",
       "0    MOND is a theory that eliminates the observed ...      D    0   \n",
       "1    Dynamic scaling refers to the evolution of sel...      A    1   \n",
       "2    The triskeles symbol is a representation of th...      A    2   \n",
       "3    Regularizing the mass-energy of an electron wi...      C    3   \n",
       "4    The angular spacing of features in the diffrac...      D    4   \n",
       "..                                                 ...    ...  ...   \n",
       "195  The three moment theorem is used to derive the...      C  195   \n",
       "196  The throttling process is a steady adiabatic f...      B  196   \n",
       "197  The excess base metal will often evaporate, be...      B  197   \n",
       "198  Mass is a property that determines the size of...      D  198   \n",
       "199  Arthur Eddington showed that two of Einstein's...      C  199   \n",
       "\n",
       "                                           context_new  \n",
       "0    -MOND is an example of a class of theories kno...  \n",
       "1    -In such systems we can define a certain time-...  \n",
       "2    -Classical Antiquity The triskeles proper, com...  \n",
       "3    -Regularization: Classical physics theory brea...  \n",
       "4    -Several qualitative observations can be made ...  \n",
       "..                                                 ...  \n",
       "195  -In civil engineering and structural analysis ...  \n",
       "196  -Throttling One of the simple applications of ...  \n",
       "197  -Similarly, a hypoeutectoid alloy has two crit...  \n",
       "198  -Mass is (among other properties) an inertial ...  \n",
       "199  -The possibility of gravitational waves was di...  \n",
       "\n",
       "[200 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deberta V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from typing import Optional, Union\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebertaModel(nn.Module):\n",
    "    def __init__(self, modelname_or_path, config, dropout=0.2, pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Transformer\n",
    "        self.config = config\n",
    "        if pretrained:\n",
    "            self.transformer = AutoModelForMultipleChoice.from_pretrained(modelname_or_path, config=config)\n",
    "        else:\n",
    "            self.transformer = AutoModelForMultipleChoice.from_config(config)\n",
    "        \n",
    "\n",
    "    def _init_weights(self, module, config):\n",
    "        module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n",
    "        if module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None,token_type_ids=None):\n",
    "        out = self.transformer(input_ids, attention_mask =attention_mask, token_type_ids=token_type_ids)\n",
    "        logits = -out['logits'] \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_answering_input_deberta(\n",
    "        tokenizer, # longformer_tokenizer\n",
    "        question,  # str\n",
    "        options,   # List[str]\n",
    "        context,   # str\n",
    "        max_seq_length=4096,\n",
    "    ):\n",
    "    \n",
    "    first_sentence = [ \"[CLS] \" + context ] * 5\n",
    "    second_sentences = [\" #### \" + question + \" [SEP] \" + option + \" [SEP]\" for option in options]\n",
    "    tokenized_examples = tokenizer(first_sentence, second_sentences, truncation='only_first', \n",
    "                                  max_length=max_seq_length, add_special_tokens=False)\n",
    "      \n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "\n",
    "class LlmseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.option_to_index = {option: idx for idx, option in enumerate('ABCDE')}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        example = self.df.iloc[idx]\n",
    "        tokenized_example = dict()\n",
    "        \n",
    "        options = [ example[option] for option in 'ABCDE']\n",
    "\n",
    "        tokenized_example = prepare_answering_input_deberta(tokenizer=self.tokenizer, question=example['prompt'], options=options, context= example['context'], max_seq_length = MAX_CONTEXT )\n",
    "\n",
    "        return tokenized_example\n",
    "            \n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        \n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "        \n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_NAME = 'models/microsoft/deberta-v3-large/'\n",
    "MODEL_PATH = 'models/microsoft/deberta-v3-large/'\n",
    "base_checkpoint_path = 'base/output/deberta_w_context_debertaformat/'\n",
    "TOKENIZER_PATH = 'models/microsoft/deberta-v3-large/'\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(checkpoint_path):\n",
    "    \n",
    "    \n",
    "    model_config = AutoConfig.from_pretrained(CONFIG_NAME)\n",
    "    model = DebertaModel(MODEL_PATH, config=model_config)\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "    model.load_state_dict(torch.load((base_checkpoint_path + checkpoint_path) ,map_location=torch.device('cuda')));\n",
    "    model.to(device)\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        with tqdm(valid_dataloader, leave=False) as pbar:\n",
    "            for idx, batch in enumerate(pbar):\n",
    "                \n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                output = model(**batch)\n",
    "                probability = torch.softmax(-output, dim=-1)\n",
    "                predictions.append(probability.detach().cpu().numpy())\n",
    "                \n",
    "    del model, model_config,output,probability \n",
    "    clean_memory()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1 = -outputs1.logits[0].detach().cpu().numpy()\n",
    "        probability1 = torch.softmax(torch.tensor(-losses1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/workspace/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\n",
    "val_df = LlmseDataset(df, tokenizer)\n",
    "data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "valid_dataloader = DataLoader(\n",
    "        val_df, \n",
    "        batch_size=2, \n",
    "        shuffle=False, \n",
    "        collate_fn=data_collator,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "probability_fold_0 = get_predictions('checkpoint-fold-0/pytorch_model.bin')\n",
    "probability_fold_1 = get_predictions('checkpoint-fold-1/pytorch_model.bin')\n",
    "probability_fold_2 = get_predictions('checkpoint-fold-2/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_fold_0 = np.concatenate(probability_fold_0, axis=0)\n",
    "probability_fold_1 = np.concatenate(probability_fold_1, axis=0)\n",
    "probability_fold_2 = np.concatenate(probability_fold_2, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([0.5, 0.25, 0.25])  # Define your weights here\n",
    "pred_lists = [probability_fold_0, probability_fold_1, probability_fold_2]  # Assume list1 to list5 are your 5 lists of arrays\n",
    "\n",
    "# Stack arrays along a new dimension, then multiply by weights and sum along that dimension\n",
    "final_prob = np.sum(np.stack(pred_lists) * weights[:, np.newaxis, np.newaxis], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_labels = np.array(list(\"ABCDE\"))\n",
    "# Sort indices for each row\n",
    "sorted_indices = np.argsort(final_prob, axis=1)\n",
    "\n",
    "# Take the last 3 largest for each row and reverse\n",
    "top3_indices = sorted_indices[:, -3:][:, ::-1]\n",
    "\n",
    "# Map to labels\n",
    "top3_labels = predict_labels[top3_indices]\n",
    "\n",
    "# Convert to list of lists if needed\n",
    "all_predictions = top3_labels.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = [\" \".join(i) for i in all_predictions]\n",
    "submission = pd.DataFrame({'id': df['id'],'prediction':final_predictions})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Run Platypus2-70B\n",
    "\n",
    "To run such a large model on a single T4 GPU, we run it layer by layer and sample by sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create symlinks from kaggle datasets to fake cached model\n",
    "\n",
    "checkpoint_path = Path(\"/root/.cache/\")\n",
    "checkpoint_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for part in [1, 2]:\n",
    "    source_dir = Path(f\"/kaggle/input/platypus2-70b-instruct-part{part}\")\n",
    "    for path in source_dir.glob(\"*\"):\n",
    "        try:\n",
    "            (checkpoint_path / path.name).symlink_to(path)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for sharded llama\n",
    "\n",
    "class ShardedLlama:\n",
    "    def __init__(self, checkpoint_path, device=\"cuda:0\", dtype=torch.float16):\n",
    "        \"\"\"\n",
    "        Sharded version of LlamaForCausalLM : the model is splitted into layer shards to reduce GPU memory usage.\n",
    "        During the forward pass, the inputs are processed layer by layer, and the GPU memory is freed after each layer.\n",
    "        To avoid loading the layers multiple times, we could save all the intermediate activations in RAM, but\n",
    "        as Kaggle accelerators have more GPU memory than CPU, we simply batch the inputs and keep them on the GPU.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        checkpoint_path : str or Path\n",
    "            path to the checkpoint\n",
    "        device : str, optional\n",
    "            device, by default \"cuda:0\"\n",
    "        dtype : torch.dtype, optional\n",
    "            dtype, by default torch.float16\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save parameters\n",
    "        self.checkpoint_path = Path(checkpoint_path)\n",
    "        self.device = device \n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Create model\n",
    "        self.config = AutoConfig.from_pretrained(self.checkpoint_path)\n",
    "        # For flash attention when Turing architecture will be supported : https://github.com/Dao-AILab/flash-attention/issues/542\n",
    "        # self.config.auto_map = {\"AutoModelForCausalLM\" : \"togethercomputer/LLaMA-2-7B-32K--modeling_flash_llama.LlamaForCausalLM\"} \n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        self.init_model()\n",
    "        self.layer_names = [\"model.embed_tokens\"] + [f\"model.layers.{i}\" for i in range(len(self.model.model.layers))] + [\"model.norm\", \"lm_head\"]\n",
    "\n",
    "    def init_model(self):\n",
    "    \n",
    "        # Load meta model (no memory used)\n",
    "        with init_empty_weights():\n",
    "            self.model = AutoModelForCausalLM.from_config(self.config, trust_remote_code=True)\n",
    "            self.model.tie_weights()\n",
    "            \n",
    "        self.layers = [self.model.model.embed_tokens] + list(self.model.model.layers) + [self.model.model.norm, self.model.lm_head]\n",
    "            \n",
    "        # Move buffers to device (not that much GPU memory used)\n",
    "        for buffer_name, buffer in self.model.named_buffers():\n",
    "            set_module_tensor_to_device(self.model, buffer_name, self.device, value=buffer, dtype=self.dtype)\n",
    "\n",
    "    def load_layer(self, layer_name):\n",
    "        state_dict = load_file(self.checkpoint_path / (layer_name + \".safetensors\"), device=self.device)\n",
    "        for param_name, param in state_dict.items():\n",
    "            assert param.dtype != torch.int8, \"int8 not supported (need to add fp16_statistics)\"\n",
    "            set_module_tensor_to_device(self.model, param_name, self.device, value=param, dtype=self.dtype)\n",
    "\n",
    "    def __call__(self, inputs, output_token):\n",
    "        # inputs = [(prefix, suffix), ...] with prefix.shape[0] = 1 and suffix.shape[0] = 5\n",
    "        \n",
    "        # Reboot the model to make sure buffers are loaded and memory is clean\n",
    "        del self.model\n",
    "        clean_memory()\n",
    "        self.init_model()\n",
    "        \n",
    "       # Send batch to device\n",
    "        batch = [(prefix.to(self.device), suffix.to(self.device)) for prefix, suffix in inputs]\n",
    "        n_suffixes = len(batch[0][1])\n",
    "        suffix_eos = [(suffix != self.tokenizer.pad_token_id).sum(1) - 1 for _, suffix in inputs]\n",
    "\n",
    "        # Create attention mask for the largest input, and position ids to use KV cache\n",
    "        attention_mask = torch.finfo(self.dtype).min * torch.ones(MAX_LENGTH, MAX_LENGTH)\n",
    "        attention_mask = attention_mask.triu(diagonal=1)[None, None, ...]\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "        position_ids = torch.arange(MAX_LENGTH, dtype=torch.long, device=self.device)[None, :]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor, torch.inference_mode():\n",
    "\n",
    "            # Load first layer\n",
    "            #future = executor.submit(self.load_layer, \"model.embed_tokens\")\n",
    "            self.load_layer(\"model.embed_tokens\")\n",
    "\n",
    "            for i, (layer_name, layer) in tqdm(enumerate(zip(self.layer_names, self.layers)), desc=self.device, total=len(self.layers)):\n",
    "\n",
    "                # Wait for previous layer to be loaded and load next layer\n",
    "                #future.result()\n",
    "                if (i + 1) < len(self.layer_names):\n",
    "                    #future = executor.submit(self.load_layer, self.layer_names[i + 1])\n",
    "                    self.load_layer(self.layer_names[i + 1])\n",
    "\n",
    "                # Run layer\n",
    "                for j, (prefix, suffix) in enumerate(batch):\n",
    "                    if layer_name == \"model.embed_tokens\":\n",
    "                        batch[j] = (layer(prefix), layer(suffix))\n",
    "                    elif layer_name == \"model.norm\":\n",
    "                        # Only keep the last token at this point\n",
    "                        batch[j] = (None, layer(suffix[torch.arange(n_suffixes), suffix_eos[j]][:, None]))\n",
    "                    elif layer_name == \"lm_head\":\n",
    "                        batch[j] = layer(suffix)[:, 0, output_token].detach().cpu().numpy()\n",
    "                    else:\n",
    "                        # Run prefix\n",
    "                        len_p, len_s = prefix.shape[1], suffix.shape[1]\n",
    "                        new_prefix, (k_cache, v_cache) = layer(prefix, use_cache=True, attention_mask=attention_mask[:, :, -len_p:, -len_p:])\n",
    "                        \n",
    "                        # Run suffix\n",
    "                        pos = position_ids[:, len_p:len_p + len_s].repeat(n_suffixes, 1)\n",
    "                        attn = attention_mask[:, :, -len_s:, -len_p - len_s:].repeat(n_suffixes, 1, 1, 1)\n",
    "                        kv_cache = (k_cache.repeat(n_suffixes, 1, 1, 1), v_cache.repeat(n_suffixes, 1, 1, 1))\n",
    "                        new_suffix = layer(suffix, past_key_value=kv_cache, position_ids=pos, attention_mask=attn)[0]\n",
    "                        batch[j] = (new_prefix, new_suffix)\n",
    "\n",
    "                # Remove previous layer from memory (including buffers)\n",
    "                layer.to(\"meta\")\n",
    "                clean_memory() # proposed by CPMP\n",
    "\n",
    "        # Get scores\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model on the 2 GPUs\n",
    "\n",
    "def get_tokens(row, tokenizer):\n",
    "        system_prefix = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_prefix}\"\n",
    "        instruction = \"Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no. As a potential aid to your answer, background context from Wikipedia articles is at your disposal, even if they might not always be relevant.\"\n",
    "        input_prefix = f\"Context: {row['context'][:MAX_CONTEXT]}\\nQuestion: {row['prompt']}\\nProposed answer: \"\n",
    "        prompt_prefix = system_prefix.format(instruction=instruction, input_prefix=input_prefix)\n",
    "        prefix = tokenizer(prompt_prefix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH)[\"input_ids\"]\n",
    "        prompt_suffix = [f\"{row[letter]}\\n\\n### Response:\\n\" for letter in \"ABCDE\"]\n",
    "        suffix = tokenizer(prompt_suffix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH, padding=True)[\"input_ids\"][:, 1:]\n",
    "        return prefix, suffix\n",
    "\n",
    "def run_model(device, df):\n",
    "    model = ShardedLlama(checkpoint_path, device=f\"cuda:{device}\")\n",
    "    f = partial(get_tokens, tokenizer=model.tokenizer)\n",
    "    inputs = df.apply(f, axis=1).values\n",
    "    batches = np.array_split(inputs, N_BATCHES)\n",
    "    outputs = []\n",
    "    for i, batch in enumerate(batches):\n",
    "        # Token #4874 is yes.\n",
    "        outputs += model(batch, output_token=4874)\n",
    "    return outputs\n",
    "\n",
    "# Run model\n",
    "if IS_TEST_SET: \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        outputs = list(executor.map(run_model, [0, 1], np.array_split(df, 2)))\n",
    "        outputs = sum(outputs, [])\n",
    "        \n",
    "    # Save results\n",
    "    n = len(df)\n",
    "    for i, scores in enumerate(outputs):\n",
    "        top3 = np.argsort(scores)[::-1]\n",
    "        df.loc[i, \"prediction\"] = \" \".join([\"ABCDE\"[j] for j in top3])\n",
    "    \n",
    "    # Display performances if train set is used (in this case use IS_TEST_SET=True !)\n",
    "    if \"answer\" in df.columns:\n",
    "        for i in range(n):\n",
    "            df.loc[i, \"top_1\"] = df.loc[i, \"prediction\"][0]\n",
    "            df.loc[i, \"top_2\"] = df.loc[i, \"prediction\"][2]\n",
    "            df.loc[i, \"top_3\"] = df.loc[i, \"prediction\"][4]\n",
    "\n",
    "        top_i = [(df[f\"top_{i}\"] == df[\"answer\"]).sum() for i in [1, 2, 3]]\n",
    "        print(f\"top1 : {top_i[0]}/{n}, top2 : {top_i[1]}/{n}, top3 : {top_i[2]}/{n} (total={sum(top_i)} / {n})\")\n",
    "        print(f\"Accuracy: {100*top_i[0]/n:.1f}%, map3: {100*(top_i[0] + top_i[1]*1/2 + top_i[2]*1/3).sum()/n:.1f}%\")\n",
    "else:\n",
    "    df[\"prediction\"] = \"A B C\"\n",
    "\n",
    "df[[\"prediction\"]].to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
